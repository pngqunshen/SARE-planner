{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "from AREgym import AREEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' temporarily here, to be move out to a diff file'''\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(72, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(72, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Parameters()\n",
    "    \n",
    "    def act(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value\n",
    "    \n",
    "    # used for deployment\n",
    "    def forward(self, x):\n",
    "        return self.actor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, env, network, optimizer, device=torch.device('cpu')):\n",
    "        self.env = env\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def work(self):\n",
    "        global max_episodes, max_episode_length\n",
    "\n",
    "        for i in range(max_episodes):\n",
    "            # initialize episode buffer [obs, action, reward, probability-distribution, value]??\n",
    "            observations_buffer = []\n",
    "            actions_buffer = []\n",
    "            rewards_buffer = []\n",
    "            values_buffer = []\n",
    "            log_probs_buffer = []\n",
    "\n",
    "            # sum up total reward for episode, for back calculating return ground truth\n",
    "            episode_reward = 0\n",
    "\n",
    "            # reset env\n",
    "            obs_np = self.env.reset()\n",
    "            for j in range(max_episode_length):\n",
    "                # convert observation from np array to torch tensor\n",
    "                obs = torch.tensor(obs_np, device=self.device)\n",
    "\n",
    "                # pass observation into network and get probability distribution and value\n",
    "                dist, value = self.network.act(obs)\n",
    "\n",
    "                # sample probability distribution to get action\n",
    "                action = dist.sample()\n",
    "\n",
    "                # step env and collect data\n",
    "                obs_new, reward, done = self.env.step(action)\n",
    "\n",
    "                # add data to rollout\n",
    "                reward = torch.tensor(reward, device=self.device)\n",
    "                observations_buffer.append(obs)\n",
    "                actions_buffer.append(action)\n",
    "                rewards_buffer.append(reward)\n",
    "                values_buffer.append(value)\n",
    "                log_probs_buffer.append(dist.log_prob(action))\n",
    "\n",
    "                obs = obs_new\n",
    "\n",
    "                # train\n",
    "                if TRAINING and (i % BATCH_SIZE == 0 or done):\n",
    "                    if len(observations_buffer) >= BATCH_SIZE:\n",
    "                        observations = observations_buffer[-BATCH_SIZE:]\n",
    "                        actions = actions_buffer[-BATCH_SIZE:]\n",
    "                        rewards = rewards_buffer[-BATCH_SIZE:]\n",
    "                        values = values_buffer[-BATCH_SIZE:]\n",
    "                        log_probs = log_probs_buffer[-BATCH_SIZE:]\n",
    "                    else:\n",
    "                        observations = observation_buffer[:]\n",
    "                        actions = actions_buffer[:]\n",
    "                        rewards = rewards_buffer[:]\n",
    "                        values = values_buffer[:]\n",
    "                        log_probs = log_probs_buffer[:]\n",
    "\n",
    "                    # calc return \n",
    "                    if done:\n",
    "                        bootstrapped_value = torch.tensor([[0]]).to(self.device)\n",
    "                    else:\n",
    "                        _, bootstrapped_value = self.network.act(obs)\n",
    "                    returns = self.calc_returns(rewards, bootstrapped_value)\n",
    "                    advantages = returns - values\n",
    "\n",
    "                    # split batch into minibatch and train update gradient for up to K epochs\n",
    "                    ''' how to split minibatch and choose epochs\n",
    "                    can i just take K_epochs to be batch // minibatch and iterate all minibatches in a random order?\n",
    "                    or just randomly sample from the batch to form minibatches K number of times'''\n",
    "                    for _ in range(K_EPOCHS):\n",
    "                        minibatch_start = random.randint[0: BATCHSIZE - MINIBATCH_SIZE]\n",
    "                        self.ppo(observations[minibatch_start: minibatch_start + MINIBATCH_SIZE], \n",
    "                                actions[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                returns[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                values[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                log_probs[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                advantages[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                clip=CLIP)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    def ppo(self, observations, actions, returns, values, log_probs, advantages, clip=0.3):\n",
    "        dist, value = self.network.act(observations)\n",
    "        entropy = -0.01 * dist.entropy().mean()\n",
    "        new_log_prob = dist.log_prob(actions)\n",
    "\n",
    "        ratio = torch.exp(new_log_prob - log_probs) \n",
    "        # this expands to (new prob / old prob) but pytorch doesnt have a prob function only a log prob so some gymnastics has to be done \n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio , 1 - clip, 1 + clip) * advantages\n",
    "        actor_loss = -1 *  torch.min(surr1, surr2)\n",
    "\n",
    "        # mse of estimated value and return \n",
    "        value_loss =  0.5 * (returns - values).pow(2).mean() \n",
    "\n",
    "        loss = actor_loss + value_loss + entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def calc_returns(self, rewards, bootstrapped_value):\n",
    "        '''some GAE (Generalized Advantage Estimation) thing? havent figured out whats the best way to do this yet'''\n",
    "        return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MAGIC NUMBERS'''\n",
    "\n",
    "TRAINING = True\n",
    "\n",
    "max_episodes = 1000\n",
    "max_episode_length = 250\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "MINIBATCH_SIZE = 10 \n",
    "\n",
    "CLIP = 0.2\n",
    "K_EPOCHS = 50\n",
    "\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "network = MLP()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=LR)\n",
    "env = AREEnv(500, 20)\n",
    "\n",
    "worker = Worker(env, network, optimizer, device)\n",
    "worker.work()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me5406-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
