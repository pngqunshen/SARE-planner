{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "from AREgym import AREEnv\n",
    "import gym\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' temporarily here, to be move out to a diff file'''\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            # nn.Linear(72, 256),\n",
    "            nn.Linear(3, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(3, 256),\n",
    "            # nn.Linear(72, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.log_std = 0.6 * torch.ones(1, 1)\n",
    "    \n",
    "    def act(self, x):\n",
    "        value = self.critic(x)\n",
    "        mean    = self.actor(x)\n",
    "        \n",
    "        std   = 0.2 * torch.ones(1, 1)\n",
    "        dist  = Normal(mean, std)\n",
    "        return dist, value\n",
    "    \n",
    "    # used for deployment\n",
    "    def forward(self, x):\n",
    "        return self.actor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MAGIC NUMBERS'''\n",
    "\n",
    "TRAINING = True\n",
    "\n",
    "max_episodes = 1000\n",
    "max_episode_length = 200\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "MINIBATCH_SIZE = 50\n",
    "\n",
    "CLIP = 0.2\n",
    "K_EPOCHS = 10\n",
    "\n",
    "LR = 0.00001 \n",
    "\n",
    "gamma = 0.99 #\n",
    "lamda = 0.95 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, env, network, optimizer, device=torch.device('cpu')):\n",
    "        self.env = env\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "\n",
    "    def work(self):\n",
    "        global max_episodes, max_episode_length\n",
    "\n",
    "        for i in range(max_episodes):\n",
    "            # initialize episode buffer [obs, action, reward, probability-distribution, value]??\n",
    "            # print(\"starting episode\", i)\n",
    "            observations_buffer = [] # not normalized\n",
    "            actions_buffer = [] # normalized (-1, 1)\n",
    "            rewards_buffer = [] # not normalized\n",
    "            values_buffer = [] # normalized (-1 ,1)\n",
    "            log_probs_buffer = [] #normalized i think\n",
    "\n",
    "            # reset env\n",
    "            obs_np = self.env.reset()\n",
    "            # print(obs_np)\n",
    "            for j in range(max_episode_length):\n",
    "                \n",
    "\n",
    "                # convert observation from np array to torch tensor\n",
    "                obs = torch.tensor(obs_np, device=self.device,dtype=torch.float32) / 8\n",
    "\n",
    "                # pass observation into network and get probability distribution and value\n",
    "                with torch.no_grad():\n",
    "                    dist, value = self.network.act(obs)\n",
    "\n",
    "                # sample probability distribution to get action\n",
    "                action = dist.sample()\n",
    "\n",
    "                # step env and collect data\n",
    "                obs_new, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # convert to tensor\n",
    "                obs_new = torch.tensor(obs_new, device=self.device,dtype=torch.float32)\n",
    "                reward = torch.tensor(reward, device=self.device,dtype=torch.float32)\n",
    "\n",
    "                # print(reward)\n",
    "                observations_buffer.append(obs)\n",
    "                actions_buffer.append(action)\n",
    "                rewards_buffer.append(reward)\n",
    "                values_buffer.append(value)\n",
    "                log_probs_buffer.append(dist.log_prob(action))\n",
    "\n",
    "                obs = obs_new\n",
    "\n",
    "                # train\n",
    "                if TRAINING and (j % BATCH_SIZE == 0 or done) and (j != 0):\n",
    "                    if len(observations_buffer) >= BATCH_SIZE:\n",
    "                        observations = observations_buffer[-BATCH_SIZE:]\n",
    "                        actions = actions_buffer[-BATCH_SIZE:]\n",
    "                        rewards = rewards_buffer[-BATCH_SIZE:]\n",
    "                        values = values_buffer[-BATCH_SIZE:]\n",
    "                        log_probs = log_probs_buffer[-BATCH_SIZE:]\n",
    "                    else:\n",
    "                        observations = observations_buffer[:]\n",
    "                        actions = actions_buffer[:]\n",
    "                        rewards = rewards_buffer[:]\n",
    "                        values = values_buffer[:]\n",
    "                        log_probs = log_probs_buffer[:]\n",
    "                    \n",
    "                    observations = torch.stack(observations)\n",
    "                    actions = torch.stack(actions).squeeze(1)\n",
    "                    rewards = torch.stack(rewards)\n",
    "                    values = torch.stack(values)\n",
    "                    log_probs = torch.stack(log_probs).squeeze(1)\n",
    "\n",
    "                    # calc return \n",
    "                    if done:\n",
    "                        bootstrapped_value = torch.tensor([0]).to(self.device)\n",
    "                    else:\n",
    "                        _, bootstrapped_value = self.network.act(obs)\n",
    "                    advantages, returns = self.calc_returns_gae(rewards , values, bootstrapped_value)\n",
    "    \n",
    "                    # split batch into minibatch and train update gradient for up to K epochss\n",
    "                    for _ in range(K_EPOCHS):\n",
    "                        minibatch_start = random.randint(0, BATCH_SIZE - MINIBATCH_SIZE)\n",
    "                        self.ppo(observations[minibatch_start: minibatch_start + MINIBATCH_SIZE], \n",
    "                                actions[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                returns[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                values[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                log_probs[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                advantages[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                clip=CLIP)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            if i % 10 == 0:\n",
    "                self.writer.add_scalar('reward', sum(rewards_buffer) / len(rewards_buffer), i)\n",
    "                # self.writer.add_scalar('loss',self.loss, i)\n",
    "                self.writer.add_scalar('loss/actor', self.p_l, i)\n",
    "                self.writer.add_scalar('loss/critic', self.v_l, i)\n",
    "                self.writer.add_scalar('loss/entropy', self.e_l, i)\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                torch.save(self.network.state_dict(), 'policy.pt')\n",
    "                # print(rewards[-1])\n",
    "\n",
    "\n",
    "                # render images\n",
    "                self.env.render()\n",
    "\n",
    "            print(\"done with episode \", i)\n",
    "\n",
    "    def ppo(self, observations, actions, returns, values, log_probs, advantages, clip=0.3):\n",
    "        dist, values = self.network.act(observations)\n",
    "\n",
    "        entropy = -0.01 * dist.entropy().mean()\n",
    "\n",
    "        new_log_prob = dist.log_prob(actions)\n",
    "\n",
    "        ratio = torch.exp(new_log_prob - log_probs.detach())\n",
    "\n",
    "        # this expands to (new prob / old prob) but pytorch doesnt have a prob function only a log prob so some gymnastics has to be done \n",
    "        surr1 = ratio * advantages.detach()\n",
    "        surr2 = torch.clamp(ratio , 1 - clip, 1 + clip) * advantages.detach()\n",
    "        actor_loss = -1 *  (torch.min(surr1, surr2)).mean()\n",
    "\n",
    "        # mse of estimated value and return \n",
    "        value_loss =  0.5 * (returns.detach() - values).pow(2).mean() \n",
    "\n",
    "        loss = (actor_loss + value_loss + entropy)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # this is for tensorboard\n",
    "        self.v_l = value_loss.clone().detach().cpu().numpy()\n",
    "        self.p_l = actor_loss.clone().detach().cpu().numpy()\n",
    "        self.e_l = entropy.clone().detach().cpu().numpy()\n",
    "        \n",
    "    ''' almost defo correct'''\n",
    "    def calc_returns_gae(self, rewards, values, last_value):\n",
    "\n",
    "        # scale rewards so that returns is between -1 and 0 or some shit idk\n",
    "        # rewards = rewards / 100\n",
    "        # print(\"rewards\",rewards)\n",
    "\n",
    "        last_value = torch.unsqueeze(last_value, 1)\n",
    "        values = torch.cat((values, last_value), 0)\n",
    "        advantages = torch.zeros(len(rewards) + 1)\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + (gamma * values[i + 1]) - values[i]\n",
    "            advantages[i] = delta + (gamma * lamda * advantages[i + 1]) \n",
    "        advantages = torch.unsqueeze(advantages, 1)\n",
    "        returns = advantages + values\n",
    "        # print(\"returns\", returns)\n",
    "        return advantages, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "network = MLP()\n",
    "# optimizer = torch.optim.Adam(network.parameters(), lr=LR)\n",
    "optimizer = torch.optim.Adam([{\"params\":network.actor.parameters(), \"lr\":LR}, {\"params\":network.critic.parameters(), \"lr\":LR}])\n",
    "# env = AREEnv(500, 20)\n",
    "env = gym.make('Pendulum-v0', g=3)\n",
    "\n",
    "worker = Worker(env, network, optimizer, device)\n",
    "worker.work()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = MLP()\n",
    "policy.load_state_dict(torch.load('policy.pt'))\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to test env againt policy\n",
    "env = gym.make('Pendulum-v0', g=3)\n",
    "\n",
    "obs =  env.reset()# print(obs)\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        # print(i)\n",
    "        # print(obs)\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        action = policy(obs)\n",
    "        print(action)\n",
    "        # action = [random.uniform(-1, 1)]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me5406-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
