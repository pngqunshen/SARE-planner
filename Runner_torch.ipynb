{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "from AREgym import AREEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' temporarily here, to be move out to a diff file'''\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(72, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(72, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, 1))\n",
    "    \n",
    "    def act(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value\n",
    "    \n",
    "    # used for deployment\n",
    "    def forward(self, x):\n",
    "        return self.actor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, env, network, optimizer, device=torch.device('cpu')):\n",
    "        self.env = env\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def work(self):\n",
    "        global max_episodes, max_episode_length\n",
    "\n",
    "        for i in range(max_episodes):\n",
    "            # initialize episode buffer [obs, action, reward, probability-distribution, value]??\n",
    "            observations_buffer = []\n",
    "            actions_buffer = []\n",
    "            rewards_buffer = []\n",
    "            values_buffer = []\n",
    "            log_probs_buffer = []\n",
    "\n",
    "            # sum up total reward for episode, for back calculating return ground truth\n",
    "            episode_reward = 0\n",
    "\n",
    "            # reset env\n",
    "            obs_np = self.env.reset()\n",
    "            for j in range(max_episode_length):\n",
    "                # convert observation from np array to torch tensor\n",
    "                obs = torch.tensor(obs_np, device=self.device,dtype=torch.float32)\n",
    "\n",
    "                # pass observation into network and get probability distribution and value\n",
    "                with torch.no_grad():\n",
    "                    dist, value = self.network.act(obs)\n",
    "\n",
    "                # sample probability distribution to get action\n",
    "                action = dist.sample()\n",
    "\n",
    "                # step env and collect data\n",
    "                obs_new, reward, done = self.env.step(action)\n",
    "\n",
    "                # add data to rollout\n",
    "                reward = torch.tensor(reward, device=self.device)\n",
    "                \n",
    "                observations_buffer.append(obs)\n",
    "                actions_buffer.append(action)\n",
    "                rewards_buffer.append(reward)\n",
    "                values_buffer.append(value)\n",
    "                log_probs_buffer.append(dist.log_prob(action))\n",
    "\n",
    "                obs = obs_new\n",
    "\n",
    "                # train\n",
    "                if TRAINING and (i % BATCH_SIZE == 0 or done):\n",
    "                    if len(observations_buffer) >= BATCH_SIZE:\n",
    "                        observations = observations_buffer[-BATCH_SIZE:]\n",
    "                        actions = actions_buffer[-BATCH_SIZE:]\n",
    "                        rewards = rewards_buffer[-BATCH_SIZE:]\n",
    "                        values = values_buffer[-BATCH_SIZE:]\n",
    "                        log_probs = log_probs_buffer[-BATCH_SIZE:]\n",
    "                    else:\n",
    "                        observations = observations_buffer[:]\n",
    "                        actions = actions_buffer[:]\n",
    "                        rewards = rewards_buffer[:]\n",
    "                        values = values_buffer[:]\n",
    "                        log_probs = log_probs_buffer[:]\n",
    "\n",
    "                    # calc return \n",
    "                    if done:\n",
    "                        bootstrapped_value = torch.tensor([[0]]).to(self.device)\n",
    "                    else:\n",
    "                        _, bootstrapped_value = self.network.act(obs)\n",
    "                    advantages, returns = self.calc_returns(rewards, values, bootstrapped_value)\n",
    "\n",
    "                    # split batch into minibatch and train update gradient for up to K epochs\n",
    "                    ''' how to split minibatch and choose epochs\n",
    "                    can i just take K_epochs to be batch // minibatch and iterate all minibatches in a random order?\n",
    "                    or just randomly sample from the batch to form minibatches K number of times'''\n",
    "                    for _ in range(K_EPOCHS):\n",
    "                        minibatch_start = random.randint[0: BATCHSIZE - MINIBATCH_SIZE]\n",
    "                        self.ppo(observations[minibatch_start: minibatch_start + MINIBATCH_SIZE], \n",
    "                                actions[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                returns[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                values[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                log_probs[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                advantages[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                clip=CLIP)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if i % 50:\n",
    "                torch.save(self.network.state_dict(), 'policy.pt')\n",
    "\n",
    "                # render images\n",
    "                self.env.render()\n",
    "\n",
    "            print(\"done with episode \", i)\n",
    "\n",
    "    def ppo(self, observations, actions, returns, values, log_probs, advantages, clip=0.3):\n",
    "        dist, value = self.network.act(observations)\n",
    "        entropy = -0.01 * dist.entropy().mean()\n",
    "        new_log_prob = dist.log_prob(actions)\n",
    "\n",
    "        ratio = torch.exp(new_log_prob - log_probs) \n",
    "        # this expands to (new prob / old prob) but pytorch doesnt have a prob function only a log prob so some gymnastics has to be done \n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio , 1 - clip, 1 + clip) * advantages\n",
    "        actor_loss = -1 *  torch.min(surr1, surr2)\n",
    "\n",
    "        # mse of estimated value and return \n",
    "        value_loss =  0.5 * (returns - values).pow(2).mean() \n",
    "\n",
    "        loss = actor_loss + value_loss + entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def calc_returns_gae(self, rewards, values, last_value):\n",
    "        values = torch.cat(values, last_value)\n",
    "        advantages = torch.zeros(rewards.shape(0) + 1)\n",
    "        \n",
    "        for i in reversed(range(rewards.shape(0))):\n",
    "            delta = rewards[i] + (gamma * values[i + 1]) - values[i]\n",
    "            advantages[i] = delta + (gamma * lamda * advantages[i + 1]) #\n",
    "        \n",
    "        returns = advantages + values\n",
    "        return advantages, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MAGIC NUMBERS'''\n",
    "\n",
    "TRAINING = True\n",
    "\n",
    "max_episodes = 1000\n",
    "max_episode_length = 250\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "MINIBATCH_SIZE = 10 \n",
    "\n",
    "CLIP = 0.2\n",
    "K_EPOCHS = 50 \n",
    "\n",
    "LR = 0.0001 \n",
    "\n",
    "gamma = 0.99 #\n",
    "lamda = 0.95 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d0c5825a6703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-5ac2f11e5ad8>\u001b[0m in \u001b[0;36mwork\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# reset env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mobs_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;31m# convert observation from np array to torch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/SARE-planner/AREgym.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_world\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/SARE-planner/World.py\u001b[0m in \u001b[0;36mnew_world\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;31m# choose a random point that can generate a room of room_area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_x\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mroom_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_y\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mroom_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_gen_room\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroom_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflood_room\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroom_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/SARE-planner/World.py\u001b[0m in \u001b[0;36mcan_gen_room\u001b[0;34m(self, world, room_size, room_x, room_y)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcan_gen_room\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroom_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroom_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroom_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         return not world[room_x:room_x+room_size,room_y:room_y+room_size].any() and (\\\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mroom_y\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroom_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mroom_x\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mroom_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroom_y\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mroom_y\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mroom_size\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_y\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroom_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mroom_x\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mroom_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroom_y\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mroom_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "network = MLP()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=LR)\n",
    "env = AREEnv(500, 20)\n",
    "\n",
    "worker = Worker(env, network, optimizer, device)\n",
    "worker.work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me5406-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
