{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "from AREgym import AREEnv\n",
    "import gym\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' temporarily here, to be move out to a diff file'''\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            # nn.Linear(72, 256),\n",
    "            nn.Linear(20, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(20, 256),\n",
    "            # nn.Linear(72, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.log_std = 0.6 * torch.ones(1, 1)\n",
    "        self.log_std_min = 0.1 * torch.ones(1, 1)\n",
    "\n",
    "        self.std = nn.Sequential(\n",
    "            nn.Linear(3, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def act(self, x):\n",
    "        value = self.critic(x)\n",
    "        mean    = self.actor(x)\n",
    "        \n",
    "        # std   = torch.exp(self.std(x))\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist  = Normal(mean, std)\n",
    "        return dist, value\n",
    "    \n",
    "    # used for deployment\n",
    "    def forward(self, x):\n",
    "        return self.actor(x)\n",
    "    \n",
    "    def decay_std(self, factor):\n",
    "        self.log_std = torch.max(self.log_std_min, self.log_std - factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MAGIC NUMBERS'''\n",
    "\n",
    "TRAINING = True\n",
    "\n",
    "max_episodes = 25000\n",
    "max_episode_length = 200\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "MINIBATCH_SIZE = 100\n",
    "\n",
    "CLIP = 0.2\n",
    "K_EPOCHS = 30\n",
    "\n",
    "LR_A = 0.0001 \n",
    "LR_C = 0.0005\n",
    "gamma = 0.9 \n",
    "lamda = 0.95 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, env, network, optimizer, device=torch.device('cpu')):\n",
    "        self.env = env\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "        self.v_l = 0\n",
    "        self.e_l = 0\n",
    "        self.p_l = 0\n",
    "\n",
    "\n",
    "    def work(self):\n",
    "        global max_episodes, max_episode_length\n",
    "\n",
    "        for i in range(max_episodes):\n",
    "            # initialize episode buffer [obs, action, reward, probability-distribution, value]??\n",
    "            # print(\"starting episode\", i)\n",
    "            observations_buffer = [] # not normalized\n",
    "            actions_buffer = [] # normalized (-1, 1)\n",
    "            rewards_buffer = [] # not normalized\n",
    "            values_buffer = [] # normalized (-1 ,1)\n",
    "            log_probs_buffer = [] #normalized i think\n",
    "\n",
    "            # reset env\n",
    "            obs_np = self.env.reset()\n",
    "\n",
    "            # print(obs_np)\n",
    "            for j in range(max_episode_length):\n",
    "                \n",
    "\n",
    "                # convert observation from np array to torch tensor\n",
    "                obs = torch.tensor(obs_np, device=self.device,dtype=torch.float32) / 8\n",
    "\n",
    "                # pass observation into network and get probability distribution and value\n",
    "                with torch.no_grad():\n",
    "                    dist, value = self.network.act(obs)\n",
    "\n",
    "                # sample probability distribution to get action\n",
    "                action = torch.clamp(dist.sample(), -1, 1)\n",
    "\n",
    "                # step env and collect data\n",
    "                obs_new, reward, done = self.env.step(action.cpu().numpy()[0])\n",
    "\n",
    "                # convert to tensor\n",
    "                obs_new = torch.tensor(obs_new, device=self.device,dtype=torch.float32)\n",
    "                reward = torch.tensor(reward, device=self.device,dtype=torch.float32)\n",
    "\n",
    "                # print(reward)\n",
    "                observations_buffer.append(obs)\n",
    "                actions_buffer.append(action)\n",
    "                rewards_buffer.append(reward)\n",
    "                values_buffer.append(value)\n",
    "                log_probs_buffer.append(dist.log_prob(action))\n",
    "\n",
    "                obs = obs_new\n",
    "\n",
    "                # train\n",
    "                if TRAINING and (j % BATCH_SIZE == 0 or done) and (j != 0):\n",
    "                    if len(observations_buffer) >= BATCH_SIZE:\n",
    "                        observations = observations_buffer[-BATCH_SIZE:]\n",
    "                        actions = actions_buffer[-BATCH_SIZE:]\n",
    "                        rewards = rewards_buffer[-BATCH_SIZE:]\n",
    "                        values = values_buffer[-BATCH_SIZE:]\n",
    "                        log_probs = log_probs_buffer[-BATCH_SIZE:]\n",
    "                    else:\n",
    "                        observations = observations_buffer[:]\n",
    "                        actions = actions_buffer[:]\n",
    "                        rewards = rewards_buffer[:]\n",
    "                        values = values_buffer[:]\n",
    "                        log_probs = log_probs_buffer[:]\n",
    "                    \n",
    "                    observations = torch.stack(observations)\n",
    "                    actions = torch.stack(actions).squeeze(1)\n",
    "                    rewards = torch.stack(rewards)\n",
    "                    values = torch.stack(values)\n",
    "                    log_probs = torch.stack(log_probs).squeeze(1)\n",
    "\n",
    "                    # calc return \n",
    "                    # if done:\n",
    "                    #     bootstrapped_value = torch.tensor([0]).to(self.device)\n",
    "                    # else:\n",
    "                    _, bootstrapped_value = self.network.act(obs)\n",
    "                    advantages, returns = self.calc_returns_gae(rewards , values, bootstrapped_value)\n",
    "    \n",
    "                    # split batch into minibatch and train update gradient for up to K epochss\n",
    "                    for _ in range(K_EPOCHS):\n",
    "\n",
    "                        if len(obs) < MINIBATCH_SIZE:\n",
    "                            self.ppo(observations,\n",
    "                                     actions,\n",
    "                                     returns,\n",
    "                                     values,\n",
    "                                     log_probs,\n",
    "                                     advantages, clip=CLIP)\n",
    "                        # print(\"test\")\n",
    "                        else:\n",
    "                            minibatch_start = random.randint(0, BATCH_SIZE - MINIBATCH_SIZE)\n",
    "                            self.ppo(observations[minibatch_start: minibatch_start + MINIBATCH_SIZE], \n",
    "                                    actions[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                    returns[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                    values[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                    log_probs[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                    advantages[minibatch_start: minibatch_start + MINIBATCH_SIZE],\n",
    "                                    clip=CLIP)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            if i % 10 == 0:\n",
    "                self.writer.add_scalar('score', sum(rewards_buffer), i)\n",
    "                # self.writer.add_scalar('loss',self.loss, i)\n",
    "                self.writer.add_scalar('loss/actor', self.p_l, i)\n",
    "                self.writer.add_scalar('loss/critic', self.v_l, i)\n",
    "                self.writer.add_scalar('loss/entropy', self.e_l, i)\n",
    "\n",
    "                self.network.decay_std(0.02)\n",
    "\n",
    "\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                torch.save(self.network.state_dict(), 'policy.pt')\n",
    "\n",
    "                # render images\n",
    "                self.env.render('images')\n",
    "\n",
    "            # print(\"done with episode \", i)\n",
    "\n",
    "    def ppo(self, observations, actions, returns, values, log_probs, advantages, clip=0.3):\n",
    "        dist, values = self.network.act(observations)\n",
    "\n",
    "        entropy = -0.0001 * dist.entropy().mean()\n",
    "\n",
    "        new_log_prob = dist.log_prob(actions.detach())\n",
    "\n",
    "        ratio = torch.exp(new_log_prob - log_probs.detach())\n",
    "\n",
    "        # this expands to (new prob / old prob) but pytorch doesnt have a prob function only a log prob so some gymnastics has to be done \n",
    "        surr1 = ratio * advantages.detach()\n",
    "        surr2 = torch.clamp(ratio , 1 - clip, 1 + clip) * advantages.detach()\n",
    "        actor_loss = -1 *  (torch.min(surr1, surr2)).mean()\n",
    "\n",
    "        # mse of estimated value and return \n",
    "        value_loss =  50 * (returns.detach() - values).pow(2).mean() \n",
    "\n",
    "        loss = (actor_loss + value_loss + entropy)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # this is for tensorboard\n",
    "        # print(\"here?\")\n",
    "        self.v_l = value_loss.clone().detach().cpu().numpy()\n",
    "        self.p_l = actor_loss.clone().detach().cpu().numpy()\n",
    "        self.e_l = entropy.clone().detach().cpu().numpy()\n",
    "        \n",
    "    ''' almost defo correct'''\n",
    "    def calc_returns_gae(self, rewards, values, last_value):\n",
    "\n",
    "        # scale rewards so that returns is between -1 and 0 or some shit idk\n",
    "        # rewards = rewards / 100\n",
    "        # print(\"rewards\",rewards)\n",
    "\n",
    "        last_value = torch.unsqueeze(last_value, 1)\n",
    "        values = torch.cat((values, last_value), 0)\n",
    "        advantages = torch.zeros(len(rewards) + 1)\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + (gamma * values[i + 1]) - values[i]\n",
    "            advantages[i] = delta + (gamma * lamda * advantages[i + 1]) \n",
    "        advantages = torch.unsqueeze(advantages, 1)\n",
    "        returns = advantages + values\n",
    "        # print(\"returns\", returns)\n",
    "        return advantages[0:-1], returns[0:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-839724707003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# worker.env.step(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# worker.env.render('images')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e2f78bf5b647>\u001b[0m in \u001b[0;36mwork\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mactions_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrewards_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mvalues_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mlog_probs_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "network = MLP()\n",
    "# optimizer = torch.optim.Adam(network.parameters(), lr=LR)\n",
    "optimizer = torch.optim.Adam([{\"params\":network.actor.parameters(), \"lr\":LR_A}, {\"params\":network.critic.parameters(), \"lr\":LR_C}])\n",
    "env = AREEnv(250, 10, num_laser_scan=10, heuristic_dist=10, save_map=True)\n",
    "# env = gym.make('Pendulum-v0', g=9)\n",
    "# env = PendulumActionNormalizer(env)\n",
    "\n",
    "\n",
    "worker = Worker(env, network, optimizer, device)\n",
    "# worker.env.reset()\n",
    "# worker.env.step(1)\n",
    "# worker.env.render('images')\n",
    "worker.work()\n",
    "# env\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = MLP()\n",
    "policy.load_state_dict(torch.load('policy.pt'))\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to test env againt policy\n",
    "# env = gym.make('Pendulum-v0', g=9)\n",
    "\n",
    "# obs =  env.reset()# print(obs)\n",
    "# with torch.no_grad():\n",
    "#     for i in range(1000):\n",
    "#         # print(i)\n",
    "#         # print(obs)\n",
    "#         obs = torch.tensor(obs, dtype=torch.float32)\n",
    "#         # dist, _ = policy.act(obs)\n",
    "#         # action = dist.mean * 2\n",
    "\n",
    "#         action = policy(obs)\n",
    "#         print(action)\n",
    "#         # action = [random.uniform(-1, 1)]\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me5406-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
